---
title: "CVA: calibration #2"
subtitle: "gr2-5 public school sample"
author: "Yukie Toyama"
date: "2024-06-19"
output:
  html_document:
    toc: true
    toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(readr)
require(knitr)
require(kableExtra)
# rm(list = ls()) remove everything from the env

# read item df and create target.r with mutually exclusive values
# note inum2 was created in the csv file before importing into R.
item_df <- read_csv("data/grades-2-5/cva-stimuli-map-grade-2-5-2024-06-13.csv")
# glimpse(item_df) # slightly different FormAndItemId
# n_distinct(item_df$target)
# get_dupes(item_df, target)

item.tbl1 <- read_csv("output/item.tbl.whole.sample.csv")

item_df <- item_df %>% 
  mutate(target.r = 
           ifelse(target == "condition" & FormAndItemId == "b-3-4-18", "condition2",
           ifelse(target == "raise" & FormAndItemId == "b-3-4-10", "raise2",
           ifelse(target == "selected" & FormAndItemId == "b-5-6-19m", "selected2",
           ifelse(target == "usual" & FormAndItemId == "a-5-6-5", "usual2", target))))
         ) %>% 
  mutate(decorated.r = 
           ifelse(decorated == "adopted" & FormAndItemId == "b-5-6-19m", 
                  "adopted.m", decorated))

# merge inum from calibration 1
inum <- item.tbl1 %>% select(inum, item)
item_df<- left_join(item_df, inum)


```


```{r tam_rasch, include=FALSE}

resp_df <- read_csv("data/grades-2-5/cva-irt-input-grade-2-5-2024-06-13.csv", na = c(".", "", " "))
# glimpse(resp_df)

# get FormAndItemId from the column name 
library(data.table)
fm.id <- as.data.frame(colnames(resp_df[4:ncol(resp_df)])) %>% 
  select(FormAndItemId = "colnames(resp_df[4:ncol(resp_df)])")

# combine item info into fm.id
fm.id <- left_join(fm.id, item_df)

# run Rasch model
library("TAM")
rasch <- tam(resp_df[,-c(1:3)])


```

```{r num.objects, echo=FALSE, results='hide'}

#item difficulties

fm.id<- bind_cols(fm.id, rasch$xsi)
fm.id$item.order <- seq.int(nrow(fm.id))

#person abilities
s_df <- resp_df %>% 
  select(user.assessmentUid, runId, user.grade)
abil <- tam.wle(rasch)

s_df <- 
  bind_cols(s_df, abil) %>% 
  select(-pid)

n.item <- nrow(fm.id)
n.stdt <- nrow(resp_df)
md.idiff <- round(median(rasch$item$xsi.item), digits = 2)
mean.idiff <- round(mean(rasch$item$xsi.item), digits = 2)
sd.idiff <- round(sd(rasch$item$xsi.item), digits = 2)
min.idiff <- round(min(rasch$item$xsi.item), digits = 2)
max.idiff <- round(max(rasch$item$xsi.item), digits = 2)

```

### Wright Map

In Calibration #2, we used **a restricted response data from `r n.stdt` students in grades 2-5 in public schools for `r n.item` items**, using the Rasch model. The WrightMap shows item difficulty and ability estimates on the logit scale. Items are ordered by difficulty on the x-axis. 

Consistent with the results with the whole sample (calibration #1), the map visually shows that no items exist for the very top and the very bottom ends of the ability distribution along the y-axis.  


```{r wm, echo=FALSE, results = "hide", message =FALSE, fig.asp = 0.85, fig.width = 7, out.width = "98%"}

library(WrightMap)
library(RColorBrewer)

#re-order rows from easy to difficult for WrightMap
fm.id <- fm.id %>% arrange(xsi)

wrightMap(s_df$theta, fm.id$xsi,
          label.items = fm.id$decorated.r,
          label.items.cex = 0.5,
          show.thr.lab = FALSE,
          label.items.srt = 90,
          main.title = "WrightMap: 888 students, 81 items"
	, axis.persons = "Students"
	, axis.items = "",
	dim.names = "",
	item.prop = 0.85,
	thr.sym.pch = 16,
	dim.color = "#69b3a2",
	thr.sym.col.fg = "#404080",
	thr.sym.col.bg = "#404080")


```

### Item Difficulty

Item difficulty ranged from `r min.idiff` to `r max.idiff` logits, with mean = `r mean.idiff` and SD = `r md.idiff`. 

```{r item.sum, echo=FALSE, message = FALSE, warning = F, results = "asis"}

library(hrbrthemes)

fm.id %>% 
  summarise(meam = mean(xsi),
            sd = sd(xsi),
            min = min(xsi),
            p25 = quantile(xsi, probs = 0.25),
            p50 = quantile(xsi, probs = 0.50),
            p75 = quantile(xsi, probs = 0.75),
            max = max(xsi),
            n= n()) %>% 
  knitr::kable(., format = "html", 
               caption = "Table 1. Item difficulty", 
               digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 12, full_width = T)

```

A histogram and a boxplot below show the distribution of the item difficulty estimates. Given that the mean of the student ability is constrained to 0, the current CVA items as a set is relatively easier with the median of `r md.idiff`. The red line in the boxplot shows the mean item difficulty (`r mean.idiff`).  

```{r item.dist, echo=FALSE, message = FALSE, warning = F, results = "asis"}
fm.id %>%
  ggplot( aes(x=xsi)) +
    geom_histogram(alpha=0.6, position = 'identity',
                   color="#404080", fill="#404080") +
  scale_y_continuous(breaks=seq(0,10,2)) +
  scale_x_continuous(breaks=seq(-2.5,3.5,0.5)) +
    theme_ipsum() +
    theme(
      axis.text.x = element_text(size = 10),
      axis.text.y = element_text(size = 10)
    ) +
    xlab("item difficulty estimates (in logit)") +
    ylab("count") +
  ggtitle("Item difficulty")

fm.id %>%
  ggplot(aes(x=xsi, y="")) +
  geom_boxplot() + 
  scale_x_continuous(breaks=seq(-1.5,1.5,0.5)) +
  geom_vline(xintercept = -0.35, color = "red3") +
  xlab("item difficulty estimates (in logit)") + 
  theme_linedraw() +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) 



```

### Item Fit

Interestingly, all CVA items, including one misfitting item (decorated = "quotation") from Calibration #1, have infit values within the acceptable range (0.75-1.33), indicating that they fit to the Rasch model. In the Item Fit graph below, all except for one Infit value, are located between the blue vertical lines. However, the "quotation" item still has the highest infit value = 1.31 and its observed ICC looks problematic. 

Below are the ICCs for "quotation" (first graph), and the "critical" (the second graph).

A good explanation of item-fit statistics by Margaret Wu can be found [here](https://www.edmeasurementsurveys.com/residual-based-item-fit-statistics.html).

```{r fit, echo=FALSE, message = FALSE, warning = F, results = "hide", fig.asp = 0.95, fig.height = 6, out.width = "98%"}

# get fit stats
fit <- tam.fit(rasch) 

# combine fit stats into fm.id
fm.id <- fm.id %>% 
  arrange(item.order)
fm.id <- bind_cols(fm.id, fit$itemfit)


# fitplot
require(ggplot2)

fm.id %>% 
  ggplot(aes(x = item.order, y = Infit)) +
  geom_point(color = "#404080") +
  scale_x_continuous(breaks=fm.id$item.order, 
                     labels=fm.id$decorated.r) + 
  geom_hline(yintercept = 0.75, color = "blue3") +
  geom_hline(yintercept = 1.33, color = "blue3") +
  theme(axis.text.x=element_text(angle=90, vjust=.5, size = 0.8),
        plot.title = element_text(size = 14, face = "bold")) +
  xlab("") +
  ggtitle("Item Fit") + theme_linedraw() + coord_flip() 

```

```{r fit2, echo=FALSE, message = FALSE, warning = F, results = "hide"}

#examine expected & observed ICC for "quotation"
#note item number differs from calibratrion #1
plot(rasch,items=38)
# plot(rasch,items=45, ngroups = 3)

#examine expected & observed ICC for "critical"
#note item number differs from calibratrion #1
plot(rasch,items=55)

```

```{r fit3, echo=FALSE, message = FALSE, warning = F}

rownames(fm.id) <- NULL

# "quotation" infit value is 1.31
fm.id %>% filter(decorated == "quotation") %>% 
  select(inum, item, decorated, target,
         starts_with("distractor"), 
         difficulty = xsi ,Infit, Infit_t) %>% 
  knitr::kable(., format = "html", 
               caption = "Table 3a. Misfitting item (quotation)", 
               digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 12, full_width = T)

fm.id %>% filter(decorated == "critical") %>% 
  select(inum, item, decorated, target,
         starts_with("distractor"),
         difficulty = xsi, Infit, Infit_t) %>% 
  knitr::kable(., format = "html", 
               caption = "Table 3b. Item (critical)", 
               digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 12, full_width = T)

```

### Item Table
Table below shows the item difficulty and item fit for the 81 CVA items, in the descending order of item difficulty (i.e., the most difficulty item at the top).

```{r itemtble, echo = FALSE, message = FALSE, warning = F}

item.tbl2 <- fm.id %>% 
  select(inum, difficulty = xsi,
         Infit, item, decorated, target, 
         starts_with("distractor")) %>% 
  arrange(desc(difficulty))

library(DT)
item.tbl2%>%
  DT::datatable(rownames = FALSE,
                options = list(pageLength = 20)) %>%
  DT::formatRound(columns= c("difficulty", "Infit"), digits=2)

# write_csv(item.tbl2, "output/item.tbl.gr2_5_pubschl.sample.csv")

```

### Item Difficulty Comparison
A scatter plot below shows item difficulty estimates from calibration with the target sample vs. those with the whole sample. The correlation is high (r = 0.95) and the values are generally higher with the target sample as indicated by most dots above the y=x equality line. 

There is one stark outlier: i.08 which decorated word is "illustration" and target word "picture". The difficulty value is higher for the whole sample (0.81) than for the target grades 2-5 sample (0.11). But note that the two axes are on different scales so these values are not directly comparable. Y-axis' zero represents the mean of the target sample while that of X represents the mean of whole sample. 

```{r idiffcomp, echo=FALSE, message = FALSE, warning = F, fig.width=7, fig.height=7}

item.tbl1 <- item.tbl1 %>% arrange(inum)
item.tbl2 <- item.tbl2 %>% arrange(inum)

library(ggpubr)
ggplot(data = data.frame(x = item.tbl1$difficulty, 
                         y = item.tbl2$difficulty), 
       aes(x, y)) + 
  geom_point() +
    theme_ipsum() +
  ylab("difficulty from Target Sample") +
  xlab("difficulty from Whole Sample") +
  scale_y_continuous(breaks=seq(-1.5,2,0.5)) +
  scale_x_continuous(breaks=seq(-1.5,2,0.5)) +
  geom_abline(intercept = 0, slope = 1, linewidth = 0.5) +
  stat_cor(aes(label = ..r.label..),
           label.x = -1, label.y = 1.4) + 
  annotate("text", 
           x = 1.2, y = 0.1, 
           label = "i.08 = illustration",
           colour = "blue") +
  ggtitle("Item Difficulty from Target Sample vs. Whole Sample")

idiff.comp <- data.frame(inum = item.tbl1$inum,
                         decorated = item.tbl1$decorated,
                         target = item.tbl1$target,
                         sample.w = item.tbl1$difficulty, 
                         sample.t = item.tbl2$difficulty)

idiff.comp %>% filter(inum == "i.08")

```


### Student Ability (overall)

```{r abilities, echo=FALSE, message = FALSE, warning = F}
s_df %>% 
  summarise(meam = mean(theta),
            sd = sd(theta),
            min = min(theta),
            p25 = quantile(theta, probs = 0.25),
            p50 = quantile(theta, probs = 0.50),
            p75 = quantile(theta, probs = 0.75),
            max = max(theta),
            n= n()) %>% 
  knitr::kable(., format = "html", 
               caption = "Table 2a. Student ability (all grades)", 
               digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 12, full_width = T)
```

A histogram and a boxplot below show the distribution of the student ability estimates. As can be seen, this distribution is much more stretched across the logit scale from -2.79 to 4.14 (compared to the item difficulty range: -1.43 to 1.50). The red line in the boxplot shows the mean person ability, which is constraint to 0 for the model identification purpose. 

```{r abil.dist, echo=FALSE, message = FALSE, warning = F, results = "asis"}

s_df %>%
  ggplot( aes(x=theta)) +
    geom_histogram(alpha=0.6, position = 'identity',
                   color="#69b3a2", fill="#69b3a2") +
  # scale_y_continuous(breaks=seq(0,10,2)) +
  scale_x_continuous(breaks=seq(-2.5,3.5,0.5)) +
    theme_ipsum() +
    theme(
      axis.text.x = element_text(size = 10),
      axis.text.y = element_text(size = 10)
    ) +
    xlab("student ability estimates (in logit)") +
    ylab("count") +
  ggtitle("Student Ability")

s_df %>%
  ggplot(aes(x=theta, y="")) +
  geom_boxplot() + 
  scale_x_continuous(breaks=seq(-3.5,4.0,0.5)) +
  geom_vline(xintercept = 0, color = "red3") +
  xlab("person ability estimates (in logit)") + 
  theme_linedraw() +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) 

```

### Student Ability (by grade)

We now show the summary statistics and distributions of the ability estimates by grade levels. The redline on the boxplots indicate the overall sample mean. 

```{r abil.by.gr, echo=F, message=F, warning=F}
s_df %>%
  group_by(user.grade) %>% 
  summarise(meam = mean(theta),
            sd = sd(theta),
            min = min(theta),
            max = max(theta),
            n= n()) %>% 
  knitr::kable(., format = "html", 
               caption = "Table 2b. Student ability, by grade", 
               digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 12, full_width = T)

library(viridis)

s_df %>%
  mutate(grade_fac = as.factor(user.grade)) %>% 
  ggplot( aes(x=theta, fill=grade_fac, colour = grade_fac)) +
    geom_histogram(alpha=0.6, position = 'identity') +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
  scale_y_continuous(breaks=seq(0,60,20)) +
    theme_ipsum() +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      axis.text.x = element_text(size = 10),
      axis.text.y = element_text(size = 10)
    ) +
    xlab("ability estimates (in logit)") +
    ylab("count") +
    facet_wrap(~grade_fac, ncol = 3) +
  ggtitle("Ability estimates by grade")

s_df %>%
  mutate(grade_fac = as.factor(user.grade)) %>%
  ggplot(aes(y=theta, x=grade_fac, fill = grade_fac)) +
  geom_boxplot() + 
  scale_y_continuous(breaks=seq(-3.5,4.0,0.5)) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) + 
  geom_hline(yintercept = 0, color = "red3") +
  ylab("person ability estimates (in logit)") +
  xlab("grade") +
  theme_linedraw() +
  theme(axis.ticks.x=element_blank(),
        legend.position="none") 


```

### Distractor Analysis

In the discriminator analysis, students were divided into four groups based on their total scores. The proportions of students in each groups choosing a particular answer choice are shown in the last four columns.

We expect that the majority of upper group to choose the correct option. If a different option is more attractive to the upper group, the item and its options should be flagged for review. For item 45 (decorated word = **quotation**), the answer choices "example" and "symbol"are more attractive to all four groups. 

Note that option **zzz** indicate "missing". It includes both missing by design (i.e., students did not get this item) as well as missing as students did not answer. 

**pBis** indicates the point-biserial correlation between that response and the total score with that item removed. 

```{r distractor, echo=FALSE, message = FALSE, warning = F}

library("CTT")

inum <- item_df %>% 
  select(FormAndItemId, inum2) 

rdf<- read_csv("data/grades-2-5/cva-trials-public-grade-2-5-2024-06-13.csv") %>%
  mutate(Form = ifelse(is.na(Form), "b-5-6", Form),
         itemId = ifelse(is.na(itemId), "19m", itemId)) %>% 
  mutate(FormAndItemId = paste(Form, itemId, sep = "-item-")) %>% 
  select(user.assessmentUid, FormAndItemId, item, response, target) 

rdf <- left_join(rdf, inum)

# key needs to be formatted as matrix
cva.key <- distinct(rdf, inum2, target, .keep_all = TRUE) %>% 
  arrange(inum2) %>% select(target) %>% as.matrix()

rdf <- rdf %>% 
  select(sid = user.assessmentUid, inum2, response) %>% 
  pivot_wider(names_from = inum2, values_from = response)

rdf2 <- rdf %>% 
  replace(is.na(.), "zzz") %>% 
  select(-sid)

ws.DA <- distractorAnalysis(rdf2,cva.key)

# combine the lists in ws.DA into a dataframe
library(purrr)
ws.DA.df <- map_df(ws.DA, ~as.data.frame(.x), .id="inum2") %>% 
  group_by(inum2) %>%
  filter(n != 0)

# get decorate column
decorated <- fm.id %>% 
  select(inum2, decorated)

# merge decorate column, clean up the table
ws.DA.df <- 
  left_join(ws.DA.df, decorated) %>% 
  select(inum2, decorated, correct,
          options = key, everything()) %>% 
  mutate(across(where(is.numeric), round, 3))

# prop of respondents with that option and discrimination are meaningless w/ missing cases present so drop.
ws.DA.df <- ws.DA.df %>% 
  select(-c(rspP, discrim))

# sort the table by item number, correct = * at the top, then alphabetical by options
ws.DA.df <- ws.DA.df %>% 
  arrange(inum2, desc(correct), options)

# print the distractor analysis results table
library(DT)
ws.DA.df %>% 
  DT::datatable(rownames = FALSE,
                options = list(pageLength = 5)) 

# Results provided in a .csv file.
# write_csv(ws.DA.df, "cva.distractor.gr2-5.sample.csv")

```

